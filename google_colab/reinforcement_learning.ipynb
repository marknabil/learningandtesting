{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"reinforcement_learning.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"xBAip2m9u6N9","colab_type":"text"},"cell_type":"markdown","source":["# reinforcement learning\n","\n"]},{"metadata":{"id":"E3BFDDMfvINf","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# following the link : \n","# https://github.com/adeshpande3/ReinforcementLearning\n","# Got the practice problem from here: http://i.stack.imgur.com/JHdT2.png"],"execution_count":0,"outputs":[]},{"metadata":{"id":"tkgDP0hKu8zg","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":34},"outputId":"2db55cc1-d367-49ec-e05f-3985e0e98567","executionInfo":{"status":"ok","timestamp":1527000937428,"user_tz":-120,"elapsed":460,"user":{"displayName":"mark nabil","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"100349270716175055813"}}},"cell_type":"code","source":["import numpy as np\n","\n","# This program is to create an RL agent and see if it can learn to perform \n","# correctly in a simple environment. The environment will be a 1-D path\n","# that has a reward at one end of the path. This program uses the Monte Carlo\n","# technique for adjusting the value function. \n","\n","# Monte Carlo is different from\n","# TD in that you update the value function after each episode, not after \n","# each move. This is because we need to observe the total reward until\n","# the episode finishes, in order to make the value function update. \n","\n","# +1 reward at the right, 0 reward everywhere else\n","# If agent reaches either end, the episode terminates\n","path = np.array([0,0,0,0,0,0,1])\n","\n","class Agent:\n","\tdef __init__(self):\n","\t\tself.valueFunction = np.array([0.5,0.5,0.5,0.5,0.5,0.5,0.5]) #Value function at every position is the same, initially\n","\t\tself.curLoc = 3 #Starting index is at the middle\n","\t\tself.alphaRate = .1\n","\n","\t\t# V(s) = V(s) + alpha*(G(t) - V(s))\n","\tdef simulateEpisode(self):\n","\t\tpositionsVisited = []\n","\t\twhile True:\n","\t\t\tpositionsVisited.append(self.curLoc)\n","\t\t\tprobability = np.random.rand()\n","\t\t\tif (probability >= .5): # Go Right\n","\t\t\t\tself.curLoc += 1;\n","\t\t\t\tif (self.curLoc == 6):\n","\t\t\t\t\t# You get a +1 reward if you reach the rightmost space\n","\t\t\t\t\tself.updateValueFunction(positionsVisited, 1)\n","\t\t\t\t\treturn\t\n","\t\t\telse: # Go Left\n","\t\t\t\tself.curLoc -= 1;\n","\t\t\t\tif (self.curLoc == 0):\n","\t\t\t\t\t# You don't get any reward if you reach the leftmost space\n","\t\t\t\t\tself.updateValueFunction(positionsVisited, 0)\n","\t\t\t\t\treturn\n","\n","\tdef updateValueFunction(self, positions, reward):\n","\t\tif (reward == 1):\n","\t\t\tfor pos in positions:\n","\t\t\t\tself.valueFunction[pos] += self.alphaRate*(1 - self.valueFunction[pos])\n","\t\telse:\n","\t\t\tfor pos in positions:\n","\t\t\t\tself.valueFunction[pos] += self.alphaRate*(-self.valueFunction[pos])\n","\n","numEpisodes = 1000\n","my_agent = Agent()\n","for i in range(0,numEpisodes):\n","\tmy_agent.simulateEpisode()\n","\tmy_agent.curLoc = 3\n","\n","print (\"The value function is \", my_agent.valueFunction[1:6])"],"execution_count":7,"outputs":[{"output_type":"stream","text":["The value function is  [0.11059653 0.13697192 0.15103669 0.30226373 0.76784331]\n"],"name":"stdout"}]},{"metadata":{"id":"_C5zdUGQw8Ki","colab_type":"text"},"cell_type":"markdown","source":["# QLearning"]},{"metadata":{"id":"dCHHd1FFwydJ","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":102},"outputId":"d73b6586-25f5-4e24-818a-916747c259d4","executionInfo":{"status":"ok","timestamp":1526991132313,"user_tz":-120,"elapsed":514,"user":{"displayName":"mark nabil","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"100349270716175055813"}}},"cell_type":"code","source":["import numpy as np\n","import sys\n","# This program is to create an RL agent and see if it can learn to perform \n","# correctly in a simple environment. The environment will be a 1-D path\n","# that has a reward at one end of the path. This program uses a Q Learning\n","# approach to find the optimal policy. \n","\n","# +1 reward at the right, 0 reward everywhere else\n","# If agent reaches either end, the episode terminates\n","path = np.array([0,0,0,0,0,0,1])\n","\n","class Agent:\n","\tdef __init__(self):\n","\t\tself.QFunction = np.full((path.size, 2), .5) #Action value function at every position is the same, initially\n","\t\tself.curLoc = 3 #Starting index is at the middle\n","\t\tself.alphaRate = .1\n","\n","\t\t# Q(s,a) = Q(s,a) + alpha*(R(t+1) + argmax(Q) - Q(s,a))\n","\tdef simulateEpisode(self):\n","\t\twhile True:\n","\t\t\tprobability = np.random.rand()\n","\t\t\tepsilon = .01\n","\t\t\tif (probability <= epsilon): #Take random action (epsilon greedy policy)\n","\t\t\t\tprobability = np.random.rand()\n","\t\t\t\tif (probability >= .5):\n","\t\t\t\t\taction = 1; #Go Right\n","\t\t\t\telse:\n","\t\t\t\t\taction = 0; #Go Left\n","\t\t\telse:\n","\t\t\t\taction = np.argmax(self.QFunction[self.curLoc,:]) #Act greedily with respect to the Q function.\n","\n","\t\t\tif (action == 1): # Go Right\n","\t\t\t\tself.curLoc += 1;\n","\t\t\t\tif (self.curLoc == 6):\n","\t\t\t\t\t# You get a +1 reward if you reach the rightmost space\n","\t\t\t\t\tself.updateQFunction(self.curLoc, action, 1, -1)\n","\t\t\t\t\t#self.QFunction[self.curLoc-1, action] += self.alphaRate*\n","\t\t\t\t\t#(1 + np.argmax(self.QFunction[self.curLoc,:]) - self.QFunction[self.curLoc-1, action])\n","\t\t\t\t\treturn\n","\t\t\t\telse:\n","\t\t\t\t\tself.updateQFunction(self.curLoc, action, 0, -1)\n","\t\t\t\t\t#self.QFunction[self.curLoc-1, action] += self.alphaRate*\n","\t\t\t\t\t#(np.argmax(self.QFunction[self.curLoc,:]) - self.QFunction[self.curLoc-1, action])\n","\t\t\t\t\t\n","\t\t\telse: # Go Left\n","\t\t\t\tself.curLoc -= 1;\n","\t\t\t\tif (self.curLoc == 0):\n","\t\t\t\t\t# You don't get any reward if you reach the leftmost space\n","\t\t\t\t\tself.updateQFunction(self.curLoc, action, 0, 1)\n","\t\t\t\t\t#self.QFunction[self.curLoc+1, action] += self.alphaRate*(-self.valueFunction[self.curLoc+1])\n","\t\t\t\t\treturn\n","\t\t\t\telse:\n","\t\t\t\t\tself.updateQFunction(self.curLoc, action, 0, 1)\n","\t\t\t\t\t#self.QFunction[self.curLoc+1, action] += self.alphaRate*(self.valueFunction[self.curLoc] - self.valueFunction[self.curLoc+1]\n","\n","\n","\tdef updateQFunction(self, location, action, reward, direction):\n","\t\tself.QFunction[location + direction, action] += self.alphaRate*(reward + np.argmax(self.QFunction[location,:]) - self.QFunction[location + direction, action])\n","\n","numEpisodes = 100\n","my_agent = Agent()\n","for i in range(1,numEpisodes):\n","\tmy_agent.simulateEpisode()\n","\tmy_agent.curLoc = 3\n","\n","print (\"The value function is \", my_agent.QFunction[1:6])\n"],"execution_count":4,"outputs":[{"output_type":"stream","text":["The value function is  [[0.2657205  0.295245  ]\n"," [0.46542604 0.45      ]\n"," [0.29819745 0.99996842]\n"," [0.5042305  0.99996946]\n"," [0.45       0.99997223]]\n"],"name":"stdout"}]},{"metadata":{"id":"2MocSuTRxFO8","colab_type":"text"},"cell_type":"markdown","source":["# RandomPathTDZero"]},{"metadata":{"id":"UT6IN4WXxFk4","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":34},"outputId":"ab6694e9-1e76-44bd-891f-1d08a9091bf3","executionInfo":{"status":"ok","timestamp":1527000930966,"user_tz":-120,"elapsed":553,"user":{"displayName":"mark nabil","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"100349270716175055813"}}},"cell_type":"code","source":["import numpy as np\n","\n","# This program is to create an RL agent and see if it can learn to perform \n","# correctly in a simple environment. The environment will be a 1-D path\n","# that has a reward at one end of the path. This program uses the TD(0)\n","# technique for adjusting the value function\n","\n","# +1 reward at the right, 0 reward everywhere else\n","# If agent reaches either end, the episode terminates\n","\n","# Got the practice problem from here: http://i.stack.imgur.com/JHdT2.png\n","path = np.array([0,0,0,0,0,0,1])\n","\n","class Agent:\n","\tdef __init__(self):\n","\t\tself.valueFunction = np.array([0.5,0.5,0.5,0.5,0.5,0.5,0.5]) #Value function at every position is the same, initially\n","\t\tself.curLoc = 3 #Starting index is at the middle\n","\t\tself.alphaRate = .1\n","\n","\t\t# V(s) = V(s) + alpha*(R(t+1) + V(s+1) - V(s))\n","\tdef simulateEpisode(self):\n","\t\twhile True:\n","\t\t\tprobability = np.random.rand()\n","\t\t\tif (probability >= .5): # Go Right\n","\t\t\t\tself.curLoc += 1;\n","\t\t\t\tif (self.curLoc == 6):\n","\t\t\t\t\t# You get a +1 reward if you reach the rightmost space\n","\t\t\t\t\tself.valueFunction[self.curLoc-1] += self.alphaRate*(1 + self.valueFunction[self.curLoc] - self.valueFunction[self.curLoc-1])\n","\t\t\t\t\treturn\n","\t\t\t\telse:\n","\t\t\t\t\tself.valueFunction[self.curLoc-1] += self.alphaRate*(self.valueFunction[self.curLoc] - self.valueFunction[self.curLoc-1])\n","\t\t\t\t\t\n","\t\t\telse: # Go Left\n","\t\t\t\tself.curLoc -= 1;\n","\t\t\t\tif (self.curLoc == 0):\n","\t\t\t\t\t# You don't get any reward if you reach the leftmost space\n","\t\t\t\t\tself.valueFunction[self.curLoc+1] += self.alphaRate*(-self.valueFunction[self.curLoc+1])\n","\t\t\t\t\treturn\n","\t\t\t\telse:\n","\t\t\t\t\tself.valueFunction[self.curLoc+1] += self.alphaRate*(self.valueFunction[self.curLoc] - self.valueFunction[self.curLoc+1])\n","\n","\n","numEpisodes = 1000\n","my_agent = Agent()\n","for i in range(1,numEpisodes):\n","\tmy_agent.simulateEpisode()\n","\tmy_agent.curLoc = 3\n","\n","print (\"The value function is \", my_agent.valueFunction[1:6])\n","v_function = my_agent.valueFunction[1:6]\n","v_function = v_function/sum(v_function)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["The value function is  [0.32216381 0.4963546  0.69080181 1.01838557 1.29670835]\n"],"name":"stdout"}]}]}